{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "print(\"CUDA: %s\" % CUDA)\n",
    "\n",
    "# Functions to read in the corpus\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "UNK = w2i[\"<unk>\"]\n",
    "\n",
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            tag, words = line.lower().strip().split(\" ||| \")\n",
    "            yield ([w2i[x] for x in words.split(\" \")], t2i[tag])\n",
    "\n",
    "\n",
    "#Baca dataset\n",
    "train = list(read_dataset(\"dataset/classes/train.txt\"))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"dataset/classes/dev.txt\"))\n",
    "nwords = len(w2i)\n",
    "ntags = len(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "    \"\"\" Add zero-padding to a batch. \"\"\"\n",
    "\n",
    "    tags = [example.tag for example in batch]\n",
    "\n",
    "    # add zero-padding to make all sequences equally long\n",
    "    seqs = [example.words for example in batch]\n",
    "    max_length = max(map(len, seqs))\n",
    "    seqs = [seq + [PAD] * (max_length - len(seq)) for seq in seqs]\n",
    "\n",
    "    return seqs, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        bow = torch.sum(embeds, 1)\n",
    "        logits = self.linear(bow)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW(\n",
      "  (embeddings): Embedding(17614, 64)\n",
      "  (linear): Linear(in_features=64, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CBOW(nwords, 64, ntags)\n",
    "\n",
    "#Pindahkan model ke GPU apabila tersedia\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    \"\"\"Evaluate a model on a data set.\"\"\"\n",
    "    correct = 0.0\n",
    "    \n",
    "    for words, tag in data:\n",
    "        lookup_tensor = Variable(torch.LongTensor([words]))\n",
    "        scores = model(lookup_tensor)\n",
    "        predict = scores.data.numpy().argmax(axis=1)[0]\n",
    "\n",
    "        if predict == tag:\n",
    "            correct += 1\n",
    "\n",
    "    return correct, len(data), correct/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lintangsutawika/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/sent=1.9944, time=21.80s\n",
      "iter 0: test acc=0.2443\n",
      "iter 1: train loss/sent=1.9255, time=27.85s\n",
      "iter 1: test acc=0.2443\n",
      "iter 2: train loss/sent=1.9225, time=24.28s\n",
      "iter 2: test acc=0.2434\n",
      "iter 3: train loss/sent=1.9228, time=21.51s\n",
      "iter 3: test acc=0.2325\n",
      "iter 4: train loss/sent=1.9004, time=20.36s\n",
      "iter 4: test acc=0.2507\n",
      "iter 5: train loss/sent=1.9077, time=21.20s\n",
      "iter 5: test acc=0.2670\n",
      "iter 6: train loss/sent=1.9013, time=18.74s\n",
      "iter 6: test acc=0.2652\n",
      "iter 7: train loss/sent=1.8970, time=18.38s\n",
      "iter 7: test acc=0.2470\n",
      "iter 8: train loss/sent=1.8788, time=20.04s\n",
      "iter 8: test acc=0.2652\n",
      "iter 9: train loss/sent=1.8940, time=20.37s\n",
      "iter 9: test acc=0.2534\n",
      "iter 10: train loss/sent=1.8842, time=19.17s\n",
      "iter 10: test acc=0.2443\n",
      "iter 11: train loss/sent=1.8684, time=21.49s\n",
      "iter 11: test acc=0.2879\n",
      "iter 12: train loss/sent=1.8772, time=19.58s\n",
      "iter 12: test acc=0.2552\n",
      "iter 13: train loss/sent=1.8736, time=19.57s\n",
      "iter 13: test acc=0.2779\n",
      "iter 14: train loss/sent=1.8709, time=19.65s\n",
      "iter 14: test acc=0.2343\n",
      "iter 15: train loss/sent=1.8642, time=24.18s\n",
      "iter 15: test acc=0.2398\n",
      "iter 16: train loss/sent=1.8686, time=19.63s\n",
      "iter 16: test acc=0.2707\n",
      "iter 17: train loss/sent=1.8583, time=19.71s\n",
      "iter 17: test acc=0.2552\n",
      "iter 18: train loss/sent=1.8499, time=21.55s\n",
      "iter 18: test acc=0.2589\n",
      "iter 19: train loss/sent=1.8649, time=21.12s\n",
      "iter 19: test acc=0.2516\n",
      "iter 20: train loss/sent=1.8569, time=20.73s\n",
      "iter 20: test acc=0.2534\n",
      "iter 21: train loss/sent=1.8470, time=20.27s\n",
      "iter 21: test acc=0.2470\n",
      "iter 22: train loss/sent=1.8450, time=21.64s\n",
      "iter 22: test acc=0.2770\n",
      "iter 23: train loss/sent=1.8417, time=21.95s\n",
      "iter 23: test acc=0.2561\n",
      "iter 24: train loss/sent=1.8451, time=19.79s\n",
      "iter 24: test acc=0.2589\n",
      "iter 25: train loss/sent=1.8332, time=18.34s\n",
      "iter 25: test acc=0.2470\n",
      "iter 26: train loss/sent=1.8322, time=18.15s\n",
      "iter 26: test acc=0.2570\n",
      "iter 27: train loss/sent=1.8416, time=18.12s\n",
      "iter 27: test acc=0.2770\n",
      "iter 28: train loss/sent=1.8379, time=18.87s\n",
      "iter 28: test acc=0.2570\n",
      "iter 29: train loss/sent=1.8318, time=18.14s\n",
      "iter 29: test acc=0.2480\n",
      "iter 30: train loss/sent=1.8308, time=18.14s\n",
      "iter 30: test acc=0.2643\n",
      "iter 31: train loss/sent=1.8263, time=18.12s\n",
      "iter 31: test acc=0.2434\n",
      "iter 32: train loss/sent=1.8222, time=18.21s\n",
      "iter 32: test acc=0.2634\n",
      "iter 33: train loss/sent=1.8253, time=18.18s\n",
      "iter 33: test acc=0.2534\n",
      "iter 34: train loss/sent=1.8167, time=18.20s\n",
      "iter 34: test acc=0.2725\n",
      "iter 35: train loss/sent=1.8206, time=18.21s\n",
      "iter 35: test acc=0.2579\n",
      "iter 36: train loss/sent=1.8223, time=18.11s\n",
      "iter 36: test acc=0.2843\n",
      "iter 37: train loss/sent=1.8226, time=18.15s\n",
      "iter 37: test acc=0.2707\n",
      "iter 38: train loss/sent=1.8178, time=18.10s\n",
      "iter 38: test acc=0.2570\n",
      "iter 39: train loss/sent=1.8114, time=23.83s\n",
      "iter 39: test acc=0.2707\n",
      "iter 40: train loss/sent=1.8115, time=24.69s\n",
      "iter 40: test acc=0.2461\n",
      "iter 41: train loss/sent=1.8046, time=19.75s\n",
      "iter 41: test acc=0.2797\n",
      "iter 42: train loss/sent=1.8097, time=22.23s\n",
      "iter 42: test acc=0.2779\n",
      "iter 43: train loss/sent=1.8122, time=18.86s\n",
      "iter 43: test acc=0.2698\n",
      "iter 44: train loss/sent=1.8012, time=22.74s\n",
      "iter 44: test acc=0.2570\n",
      "iter 45: train loss/sent=1.8105, time=20.69s\n",
      "iter 45: test acc=0.2516\n",
      "iter 46: train loss/sent=1.8062, time=18.65s\n",
      "iter 46: test acc=0.2570\n",
      "iter 47: train loss/sent=1.8114, time=18.13s\n",
      "iter 47: test acc=0.2643\n",
      "iter 48: train loss/sent=1.7970, time=18.85s\n",
      "iter 48: test acc=0.2843\n",
      "iter 49: train loss/sent=1.8010, time=18.88s\n",
      "iter 49: test acc=0.2516\n",
      "iter 50: train loss/sent=1.7937, time=20.55s\n",
      "iter 50: test acc=0.2688\n",
      "iter 51: train loss/sent=1.7972, time=19.92s\n",
      "iter 51: test acc=0.2743\n",
      "iter 52: train loss/sent=1.7949, time=19.41s\n",
      "iter 52: test acc=0.2797\n",
      "iter 53: train loss/sent=1.7969, time=20.43s\n",
      "iter 53: test acc=0.2988\n",
      "iter 54: train loss/sent=1.7905, time=19.07s\n",
      "iter 54: test acc=0.2807\n",
      "iter 55: train loss/sent=1.7934, time=20.00s\n",
      "iter 55: test acc=0.2834\n",
      "iter 56: train loss/sent=1.7892, time=19.65s\n",
      "iter 56: test acc=0.2634\n",
      "iter 57: train loss/sent=1.7842, time=21.03s\n",
      "iter 57: test acc=0.2543\n",
      "iter 58: train loss/sent=1.7840, time=20.86s\n",
      "iter 58: test acc=0.2625\n",
      "iter 59: train loss/sent=1.7843, time=20.94s\n",
      "iter 59: test acc=0.2670\n",
      "iter 60: train loss/sent=1.7798, time=20.52s\n",
      "iter 60: test acc=0.2752\n",
      "iter 61: train loss/sent=1.7823, time=20.87s\n",
      "iter 61: test acc=0.2652\n",
      "iter 62: train loss/sent=1.7842, time=21.96s\n",
      "iter 62: test acc=0.2625\n",
      "iter 63: train loss/sent=1.7874, time=21.50s\n",
      "iter 63: test acc=0.2825\n",
      "iter 64: train loss/sent=1.7715, time=22.02s\n",
      "iter 64: test acc=0.2443\n",
      "iter 65: train loss/sent=1.7755, time=19.83s\n",
      "iter 65: test acc=0.2716\n",
      "iter 66: train loss/sent=1.7824, time=19.59s\n",
      "iter 66: test acc=0.2625\n",
      "iter 67: train loss/sent=1.7742, time=19.07s\n",
      "iter 67: test acc=0.2770\n",
      "iter 68: train loss/sent=1.7743, time=22.54s\n",
      "iter 68: test acc=0.2534\n",
      "iter 69: train loss/sent=1.7821, time=20.17s\n",
      "iter 69: test acc=0.2598\n",
      "iter 70: train loss/sent=1.7721, time=20.19s\n",
      "iter 70: test acc=0.2725\n",
      "iter 71: train loss/sent=1.7663, time=20.90s\n",
      "iter 71: test acc=0.2579\n",
      "iter 72: train loss/sent=1.7749, time=19.78s\n",
      "iter 72: test acc=0.2707\n",
      "iter 73: train loss/sent=1.7653, time=19.73s\n",
      "iter 73: test acc=0.2525\n",
      "iter 74: train loss/sent=1.7640, time=19.46s\n",
      "iter 74: test acc=0.2561\n",
      "iter 75: train loss/sent=1.7706, time=23.82s\n",
      "iter 75: test acc=0.2534\n",
      "iter 76: train loss/sent=1.7646, time=18.90s\n",
      "iter 76: test acc=0.2480\n",
      "iter 77: train loss/sent=1.7716, time=19.56s\n",
      "iter 77: test acc=0.2652\n",
      "iter 78: train loss/sent=1.7736, time=24.71s\n",
      "iter 78: test acc=0.2661\n",
      "iter 79: train loss/sent=1.7690, time=20.63s\n",
      "iter 79: test acc=0.2561\n",
      "iter 80: train loss/sent=1.7650, time=22.36s\n",
      "iter 80: test acc=0.2679\n",
      "iter 81: train loss/sent=1.7610, time=22.44s\n",
      "iter 81: test acc=0.2561\n",
      "iter 82: train loss/sent=1.7633, time=20.48s\n",
      "iter 82: test acc=0.2888\n",
      "iter 83: train loss/sent=1.7638, time=20.21s\n",
      "iter 83: test acc=0.2779\n",
      "iter 84: train loss/sent=1.7561, time=20.94s\n",
      "iter 84: test acc=0.2997\n",
      "iter 85: train loss/sent=1.7702, time=18.78s\n",
      "iter 85: test acc=0.2525\n",
      "iter 86: train loss/sent=1.7516, time=21.07s\n",
      "iter 86: test acc=0.2761\n",
      "iter 87: train loss/sent=1.7561, time=21.38s\n",
      "iter 87: test acc=0.2625\n",
      "iter 88: train loss/sent=1.7501, time=22.44s\n",
      "iter 88: test acc=0.2752\n",
      "iter 89: train loss/sent=1.7541, time=20.25s\n",
      "iter 89: test acc=0.2607\n",
      "iter 90: train loss/sent=1.7560, time=19.04s\n",
      "iter 90: test acc=0.2679\n",
      "iter 91: train loss/sent=1.7547, time=20.87s\n",
      "iter 91: test acc=0.2943\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ee3e63dc914c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for ITER in range(100):\n",
    "\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "\n",
    "    for words, tag in train:\n",
    "\n",
    "        # forward pass\n",
    "        lookup_tensor = Variable(torch.LongTensor([words]))\n",
    "        scores = model(lookup_tensor)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        target = Variable(torch.LongTensor([tag]))\n",
    "        output = loss(scores, target)\n",
    "        train_loss += output.data[0]\n",
    "\n",
    "        # backward pass\n",
    "        model.zero_grad()\n",
    "        output.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % \n",
    "          (ITER, train_loss/len(train), time.time()-start))\n",
    "\n",
    "    # evaluate\n",
    "    _, _, acc = evaluate(model, dev)\n",
    "    print(\"iter %r: test acc=%.4f\" % (ITER, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deeper CBOW\n",
    "Tambahkan hidden layer, coba bandingkan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        bow = torch.sum(embeds, 1)\n",
    "        logits = self.linear(bow)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
