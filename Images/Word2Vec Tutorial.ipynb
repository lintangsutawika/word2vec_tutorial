{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Word2Vec Tutorial</center>\n",
    "<center>Kelas Pengolahan Bahasa Manusia 2017/2018</center>\n",
    "<center>Lintang Adyuta Sutawika</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from input_data import InputData\n",
    "import numpy\n",
    "from model import SkipGramModel\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path='data'):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.dataset = self.tokenize(os.path.join(path, 'ptb.train.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        self.tokenizer = RegexpTokenizer(r'\\S+')\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = list(self.tokenizer.tokenize(line))\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r') as f:\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                # words = line.split() + ['<eos>']\n",
    "                words = list(self.tokenizer.tokenize(line))\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return Variable(tensor)\n",
    "\n",
    "def get_index_of_max(input):\n",
    "    index = 0\n",
    "    for i in range(1, len(input)):\n",
    "        if input[i] > input[index]:\n",
    "            index = i \n",
    "    return index\n",
    "\n",
    "def get_max_prob_result(input, ix_to_word):\n",
    "    return ix_to_word[get_index_of_max(input)]\n",
    "\n",
    "def get_similarity(w1, w2, model):\n",
    "    e1 = model.get_word_emdedding(w1)\n",
    "    e2 = model.get_word_emdedding(w2)\n",
    "    return (e1.dot(e2) / (torch.norm(e1) * torch.norm(e2))).data.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continous Bag-of-Words Implementation\n",
    "class cbow(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_dim):\n",
    "        super(cbow, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocabulary_size, embedding_dim)\n",
    "        self.linear_output = nn.Linear(embedding_dim, vocabulary_size)\n",
    "        \n",
    "    def forward(self, imputs):\n",
    "        projection = sum(self.embeddings(inputs)).view(1,-1)\n",
    "        out = self.linear_output(projection)\n",
    "        return nn.LogSoftmax(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus()\n",
    "model = cbow(corpus.dictionary.len)\n",
    "\n",
    "loss_function = nn.NLLLoss() \n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for context, target in dataset:\n",
    "        input_context = Variable(torch.LongTensor(corpus.dictionary.word2idx[word] for word in context))\n",
    "        output_target = Variable(torch.LongTensor(corpus.dictionary.word2idx[target]))\n",
    "        \n",
    "        loss = loss_function(log_prob, target_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1301.3781.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Word_pair_relationships.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"training_data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"CBOW_Skip-gram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referensi\n",
    "1) http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
